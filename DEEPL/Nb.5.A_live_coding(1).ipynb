{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194d084-8597-432d-ba53-ee71b5efaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install names-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94583c4c-8847-43c1-b111-b356f6da9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from names_dataset import NameDataset, NameWrapper\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c1341-4e42-4370-b94b-beb57ed03978",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = NameDataset()\n",
    "names = nd.get_top_names(country_alpha2='AT')['AT']\n",
    "names = names['M'] + names['F']\n",
    "letters = sorted(list(set(''.join(names).lower())))\n",
    "lookup = {}\n",
    "for i in range(len(letters)):\n",
    "    lookup[letters[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b48a5-a2d9-42df-81d4-7cff398bb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDs(Dataset):\n",
    "    def __init__(self, names: list, seq_len: int, lookup: dict = None):\n",
    "        letters = sorted(list(set(''.join(names).lower())))\n",
    "        self.seq_len = seq_len\n",
    "        self.names = names\n",
    "        self.start_token = 'bos'\n",
    "        self.end_token = 'eos'\n",
    "        self.padding_token = 'pad'\n",
    "        self.lookup = lookup\n",
    "        if lookup is None:\n",
    "            self.lookup = {}\n",
    "            for i in range(len(letters)):\n",
    "                self.lookup[letters[i]] = i\n",
    "            self.lookup[self.start_token] = len(letters)\n",
    "            self.lookup[self.end_token] = len(letters) + 1\n",
    "            self.lookup[self.padding_token] = len(letters) + 2\n",
    "        \n",
    "            self.idx2letter = self._idx2letter()\n",
    "        \n",
    "    \n",
    "    def _idx2letter(self):\n",
    "        return {self.lookup[k]:k for k in self.lookup}\n",
    "    \n",
    "    def num_letters(self):\n",
    "        return len(self.lookup)\n",
    "    \n",
    "    def get_start_token(self):\n",
    "        return self.lookup[self.start_token]\n",
    "    \n",
    "    def get_end_token(self):\n",
    "        return self.lookup[self.end_token]\n",
    "        \n",
    "    def get_padding_token(self):\n",
    "        return self.lookup[self.padding_token]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def idx2name(self, idx):\n",
    "        ret = []\n",
    "        for i in idx:\n",
    "            if i == self.get_end_token():\n",
    "                break\n",
    "            elif i == self.get_start_token():\n",
    "                continue\n",
    "            ret.append(self.idx2letter[i.item()])\n",
    "        return ''.join(ret)\n",
    "    \n",
    "    def name2idx(self, name):\n",
    "        chars = list(name)\n",
    "        ret = [self.lookup[self.start_token]] + [self.lookup[c] for c in chars] + [self.lookup[self.end_token]]\n",
    "        assert self.seq_len >= len(ret), 'sequnce length exceeds maximal sequence length'\n",
    "        ret = ret + [self.lookup[self.padding_token]] * (self.seq_len - len(ret))  \n",
    "        ret = torch.tensor(ret).long()\n",
    "        return ret\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx].lower()\n",
    "        ret = self.name2idx(name)\n",
    "        return ret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df892b-b333-4556-841b-a9462ddc0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_letters, seq_len):\n",
    "        super(NameGenerator, self).__init__()\n",
    "        self.num_letters = # TODO\n",
    "        self.seq_len = # TODO\n",
    "        self.hidden_dim = # TODO\n",
    "        self.word_embeddings = # TODO\n",
    "        self.lstm = # TODO\n",
    "        self.out_layer= # TODO\n",
    "        \n",
    "    def get_init_hc(self, batch_size):\n",
    "        h = # TODO\n",
    "        c = # TODO\n",
    "        return h,c\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return out\n",
    "    \n",
    "    def generate(self, bos_idx, eos_idx, read_idx=None, max_iter=10, from_top_k=1):\n",
    "        seq = []\n",
    "        if read_idx is None:\n",
    "            seq.append((torch.ones(1)*bos_idx).long())\n",
    "            (h,c) = self.get_init_hc(1)\n",
    "            for i in range(max_iter):\n",
    "                # TODO\n",
    "        else:\n",
    "            assert read_idx[0,0] == bos_idx, f'invalid beginning of sequence! Got {read_idx[0]} but expected {bos_idx}'\n",
    "            # Add for homework\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdd9fd-5b5c-4aa9-9ab6-484f5c03ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(names, p_train):\n",
    "    n_train = int(len(names) * p_train)\n",
    "    # shuffle data\n",
    "    idx = random.sample(range(len(names)), len(names))\n",
    "    train_data = [names[i] for i in idx[:n_train]]\n",
    "    val_data = [names[i] for i in idx[n_train:]]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3ee6d-a6f1-47b2-afe1-4b031e053e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "\n",
    "# create split\n",
    "train_split, val_split = split_data(names, 0.9)\n",
    "\n",
    "# create datasets\n",
    "train_data = NamesDs(train_split, seq_len)\n",
    "val_data = NamesDs(val_split, seq_len, lookup=train_data.lookup)\n",
    "\n",
    "# create loader\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a71f53-7b2a-42da-8872-1113160e31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, dataloader, model, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_ls = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # TODO\n",
    "    return loss.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cae72-7f43-4aef-af00-3d9b507414f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch, dataloader, model, loss_func):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    n_samples = 0\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # TODO     \n",
    "    avg_loss = loss_sum/n_samples\n",
    "    return avg_loss                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be95625-a0d4-474f-b4cd-b7e07d713e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_epochs=20):\n",
    "    seq_len = 50\n",
    "\n",
    "    # create split\n",
    "    train_split, val_split = split_data(names, 0.9)\n",
    "\n",
    "    # create datasets\n",
    "    train_data = NamesDs(train_split, seq_len)\n",
    "    val_data = NamesDs(val_split, seq_len)\n",
    "\n",
    "    # create loader\n",
    "    train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_data, batch_size=4, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # create loss\n",
    "    loss_fun = nn.CrossEntropyLoss(ignore_index=train_data.get_padding_token())\n",
    "    \n",
    "    # instantiate model\n",
    "    model = NameGenerator(32, 16, train_data.num_letters(), seq_len)\n",
    "    \n",
    "    # optimizer\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.1)\n",
    "    \n",
    "    epochs = n_epochs\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t_loss = train(epoch, train_loader, model, loss_fun, optim)\n",
    "        v_loss = val(epoch, val_loader, model, loss_fun)\n",
    "        print(f\"epoch: [{epoch}/{n_epochs}]: train_loss = {t_loss:.5f} | val_loss = {v_loss:.5f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e442ed8-0ab1-4571-a880-a952c295ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = main(n_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8f1f8-1fa9-4516-a084-2428075da6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(bos_idx=26, eos_idx=27, from_top_k=2)\n",
    "train_data.idx2name(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
