{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f618b9f-06ac-4ec8-bc1f-9ce7f4583fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c4c29-6fed-4462-97de-66b1f08bfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to do here!\n",
    "# Made up functions\n",
    "# Made up ground truth function\n",
    "def f(x: torch.tensor) -> torch.tensor:\n",
    "    f = torch.exp(0.001*x)*torch.sin(0.5*x)/x + torch.log(x) + torch.sin(0.1*x)\n",
    "    return f\n",
    "\n",
    "# This function generates the observations\n",
    "def obs(x: torch.tensor) -> torch.tensor:\n",
    "    fx = f(x)\n",
    "    eps = torch.randn(len(x)) * 0.5\n",
    "    y = fx + eps\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1621ef6-9e73-42b9-9cf1-d37f87c5f0ca",
   "metadata": {},
   "source": [
    "# Implement a simple fully connected network\n",
    "A[1] - A[5]\n",
    "- input dimension, output dimension as parameters)\n",
    "- three hidden layers with dimensions: 64, 32, 16\n",
    "- with ReLu activations\n",
    "- use nn.Sequential for your network definition\n",
    "- use the appropriate weight initialization (uniform or normal doesn't matter)\n",
    "\n",
    "each of the steps above counts as 1 point i.e. the implementation in total is 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0d4bc-8ee9-4fa8-9242-0886f724edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO! \n",
    "class NN(nn.Module):\n",
    "    # TODO! implement initalization here\n",
    "    def __init__\n",
    "    \n",
    "    # TODO! implement forward here\n",
    "    def forward\n",
    "\n",
    "    # TODO! implement eright initialization here    \n",
    "    def init_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9fc95-e57a-4448-915c-d10aeb0ca923",
   "metadata": {},
   "source": [
    "# Mean Absolute Error\n",
    "A[6]\n",
    "\n",
    "MSE is prone to outliers. An alternative is the Mean Absolute Error:\n",
    "$MAE =  \\sum_{i=1}^N \\| g(x_i) - y_i \\|$ where $g$ is our model and $g(x_i)$ is the model response for the $i't$ input.\n",
    "\n",
    "Implement the MAE below. You can look up notebook NB1.A - there the MSE is implemented. MAE is just slightly different! Hint: torch.abs() operates elementwise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eebd6f-6fe9-423e-b910-04fa15f1eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y: torch.tensor, gx: torch.tensor) -> torch.tensor:\n",
    "    # TODO!\n",
    "    # implement the mae here\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eebb6c-4303-4ce7-8ff7-cf1e1f0b1d1f",
   "metadata": {},
   "source": [
    "# Complete the training loop below!\n",
    "A[7] fill in the missing code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68116f06-0052-4e26-a012-7b5f0c50aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer):\n",
    "    interval = torch.linspace(0.1,100,100) \n",
    "    fx = f(interval) \n",
    "    \n",
    "    N_datapoints = 50\n",
    "    x = torch.linspace(0.1,100,N_datapoints)\n",
    "    y = obs(x)\n",
    "    \n",
    "    n_epoch = 5000\n",
    "    loss_array = []\n",
    "    fx_diff = []\n",
    "    fx_epoch = []\n",
    "\n",
    "    track_every = 500\n",
    "    for epoch in range(n_epoch):\n",
    "    \n",
    "        # set all gradients to zero\n",
    "        # TODO!\n",
    "    \n",
    "        # apply neural network on all data points\n",
    "        # TODO!\n",
    "    \n",
    "        # calculate MAE loss\n",
    "        # TODO!\n",
    "    \n",
    "        # calculate gradients\n",
    "        # TODO!\n",
    "    \n",
    "        # one step along the steppest descent\n",
    "        # TODO!\n",
    "    \n",
    "    \n",
    "        # apply model on interval to calc diff to f\n",
    "        if epoch % track_every == 0:\n",
    "             # store loss vor visualization\n",
    "            loss_array.append(loss.item())\n",
    "            # apply model on interval to calc diff to f\n",
    "            with torch.no_grad():\n",
    "                out_f = model(interval.view(-1,1))\n",
    "                diff = ((out_f.view(-1) - fx.view(-1)).abs()).mean()\n",
    "                fx_diff.append(diff.item())\n",
    "                fx_epoch.append(epoch)\n",
    "        if epoch % track_every == 0:\n",
    "            print(f'\\r[{epoch}/{n_epoch}]: loss = {loss}', end=\"\")\n",
    "            \n",
    "    return model, loss_array, fx_diff, fx_epoch, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512bed6c-f0bf-4d83-9c4a-956174b7a948",
   "metadata": {},
   "source": [
    "# Run the training 2 times\n",
    "- [A8] once with the SGD optimizer\n",
    "- [A9] once with the ADAM optimizer\n",
    "    - you can play around with the learning rates\n",
    "- [A10] currently we have not seen how to compare models. Do you see a differenc between the optimizers? Would you say one is better then the other? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83e89c-cd84-492a-bc4d-536ae0fdd0ad",
   "metadata": {},
   "source": [
    "Train model with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9b07f-7de4-440c-b7d3-7a68dc52e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1e1ca-f0a6-4c20-9363-14cf9a6915b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_array, fx_diff, fx_epoch, x, y = train(model, \n",
    "                                                   # TODO: add SGD optimizer as second argument! \n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93f094-8047-45fa-a7b6-96c8f6d5bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()    \n",
    "plt.yscale('log')\n",
    "plt.title('Loss')\n",
    "plt.plot(fx_epoch,loss_array, label='training loss')\n",
    "plt.plot(fx_epoch, fx_diff, label='diff to f')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9c334-4d56-4eca-b6bd-40127b44be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x.view(-1,1))\n",
    "interval = torch.linspace(0.1,100,100) \n",
    "fx = f(interval) \n",
    "\n",
    "plt.close()\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(interval, fx, label='f')\n",
    "plt.plot(x, out.detach().squeeze(), label='model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39461433-26e9-4124-96b1-fe10bcfb9b95",
   "metadata": {},
   "source": [
    "Train model with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9aa1d8-57c6-43d4-859d-7ee815b5b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794a738-e61a-4116-bcf9-3b9c3e9e16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_array, fx_diff, fx_epoch, x, y = train(model, \n",
    "                                                   # TODO: add ADAM optimizer as second argument! \n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2f51a-5ace-4a81-8d46-603179c9d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()    \n",
    "plt.yscale('log')\n",
    "plt.title('Loss')\n",
    "plt.plot(fx_epoch,loss_array, label='training loss')\n",
    "plt.plot(fx_epoch, fx_diff, label='diff to f')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e22123-f0b3-4499-ae3a-c21280147981",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x.view(-1,1))\n",
    "interval = torch.linspace(0.1,100,100) \n",
    "fx = f(interval) \n",
    "\n",
    "plt.close()\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(interval, fx, label='f')\n",
    "plt.plot(x, out.detach().squeeze(), label='model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47d2dd-1845-4acb-9de1-171a6f82fdf6",
   "metadata": {},
   "source": [
    "very brief: do you see a differenc between the optimizers? Would you say one is better then the other? Why? Why not?\n",
    "\n",
    "Add one or two sentences here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946adf5-d850-4df2-acc0-a7939727d7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
