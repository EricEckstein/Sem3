{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97139659-0cac-49cc-9c60-47887e181340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00eb09-696f-47bd-a3e0-2d078084de9c",
   "metadata": {},
   "source": [
    "# Hyper Parameter selection: Reproducibility and GPU\n",
    "\n",
    "\n",
    "\n",
    "## TODO\n",
    "Take HW4 and:\n",
    "- adapt the code such that it runs on the GPU (5 Points) \n",
    "- adapt the code such that the results are reproducible AND we can choose our model based on multiple runs (e.g. 5) per seed! (5 Points) \\\n",
    "  You can leave the activation functions out of the hyper parameter search!\n",
    "\n",
    "## ATTENTION!: Below is the specification of A4. Take your old Assignment A4 to solve A5! \n",
    "( \n",
    "\n",
    "Adapt the notebook in the following way:\n",
    "- Currently there is no validation function - add it!\n",
    "- The main function should have as arguments: learning_rate, batch_size, activation_function\n",
    "- Crate a grid search over this paramters\n",
    "- This means: create three lists eg: learning_rates = [0.1, 0.01, 0.001], batch_size = [8, 16, 32], activation_functions = [ReLu, Elu]\n",
    "- iterate over all combinations of the above values: i.e. for the above case this means you need to train 3x3x2 models \n",
    "- in the best case you train models with the same hyper parameter settings multiple times, each with different seed - but this is not necessary here\n",
    "- Track the learning curves of: train loss, val loss, train accuracy, val accuracy\n",
    "- based on the curves: choose a model that you think is best!\n",
    "\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7d99a-6fe6-48a8-aa77-25077fdbdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    #def __init__(self, activationFirst, activationSecond): tried with multiple Activation funcs\n",
    "    def __init__(self, activationFunc):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=16,            \n",
    "                      kernel_size=5,              \n",
    "                      stride=1,                   \n",
    "                      padding=2),                              \n",
    "            activationFunc,                      \n",
    "            nn.MaxPool2d(kernel_size=2)   \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            activationFunc,                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "        \n",
    "        self.is_conv = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output \n",
    "            \n",
    "    \n",
    "    def zero_init(self):\n",
    "        for p in self.parameters():\n",
    "            p.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01bf8e-fc78-4dc4-bcd9-052db72a2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_idxfile(path: str) -> np.array:\n",
    "    # can we do that for all files in the folder? \n",
    "    arr = idx2numpy.convert_from_file(path)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829f3cf-f0e3-4e57-a487-0e54d0ab4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: addapt the paths and load the files\n",
    "# file_path = r'/home/hubert/Lecture/data/raw/t10k-images.idx3-ubyte'\n",
    "\n",
    "notebook_path = os.path.abspath(\"A4.ipynb\")\n",
    "folder = os.path.join(os.path.dirname(notebook_path), \"idx_raw\")\n",
    "\n",
    "file_path = os.path.join(folder, \"t10k-images.idx3-ubyte\")\n",
    "data = load_idxfile(file_path)\n",
    "file_path_labels = os.path.join(folder, \"t10k-labels.idx1-ubyte\")\n",
    "labels = load_idxfile(file_path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc1cba-6eb0-4ad9-8c4e-4253d7a2951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the data set: Nothing to do here\n",
    "class MNISTDset(Dataset):\n",
    "    def __init__(self, images: np.array, labels: np.array) -> None:\n",
    "        self.images = torch.tensor(images)/255.\n",
    "        self.labels = torch.tensor(labels)\n",
    "        self.num_samples = len(self.labels)\n",
    "        \n",
    "        # nomralize to standard deviation\n",
    "        self._normalize()\n",
    "        \n",
    "        \n",
    "    def set_num_samples(self, n:int=None) -> None:\n",
    "        '''\n",
    "        Restrict numbers of samples. \n",
    "        Not necessary, but sometimes useful for model testing \n",
    "        '''\n",
    "        if n is None:\n",
    "            self.num_samples = len(self.labels)\n",
    "        else:\n",
    "            assert 0 <=  n <= len(self.labels)\n",
    "            self.num_samples = n\n",
    "            \n",
    "    def _normalize(self, mean: float=None, std: float=None):\n",
    "        if std is not None: \n",
    "            assert std > 0\n",
    "        '''Normalize data to nomral standard'''\n",
    "        self.images = self.images - self.images.mean() if mean is None else self.images - mean \n",
    "        self.images = self.images / (self.images.std() + 1e-12) if std == None else self.images / std\n",
    "        \n",
    "    def __len__(self):\n",
    "        ret = self.num_samples\n",
    "        return ret\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label, idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d51e8-13c2-41d6-a3d8-16573355a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    return (out.argmax(-1) == label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71794817-5e1e-424c-8d24-1fdf50768c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, optimizer, loss_fun, device, epoch) -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    for i, (image, targets, idx) in enumerate(train_loader):\n",
    "        # get batch size\n",
    "        bs = image.shape[0]\n",
    "            \n",
    "        # fully connected model: we need to flatten the images\n",
    "        x = image.view(bs,-1) if not model.is_conv else image.view(bs,1,28,28)\n",
    "            \n",
    "        # image to device\n",
    "        x = x.to(device)\n",
    "            \n",
    "        # zero grads\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # forward pass\n",
    "        out = model(x)\n",
    "            \n",
    "        # calc loss and gradients\n",
    "        loss = loss_fun(out, targets).mean()\n",
    "        loss.backward()\n",
    "            \n",
    "        # update\n",
    "        optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efd2d1-193c-4aa4-b019-0b394b920865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, train_loader, optimizer, loss_fun, device, epoch) -> Tuple[float, float]:\n",
    "    # TODO: adapt code beolow\n",
    "    model.train()\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    for i, (image, targets, idx) in enumerate(train_loader):\n",
    "        # get batch size\n",
    "        bs = image.shape[0]\n",
    "            \n",
    "        # fully connected model: we need to flatten the images\n",
    "        x = image.view(bs,-1) if not model.is_conv else image.view(bs,1,28,28)\n",
    "            \n",
    "        # image to device\n",
    "        x = x.to(device)\n",
    "            \n",
    "        # zero grads\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # forward pass\n",
    "        out = model(x)\n",
    "            \n",
    "        # calc loss and gradients\n",
    "        loss = loss_fun(out, targets).mean()\n",
    "        loss.backward()\n",
    "            \n",
    "        # update\n",
    "        optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6f712-b5f4-49f5-a644-c3d575e0c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCount = len(data)\n",
    "print(\"data count=\", dataCount)\n",
    "\n",
    "dataTrainCount = (int) (dataCount * 0.8)\n",
    "print(\"80% train data split count =\", dataTrainCount)\n",
    "\n",
    "# training data\n",
    "train_data = data[:dataTrainCount] # this will take a subset of the data till the 80% size of the numpy array\n",
    "train_labels = labels[:dataTrainCount] # same for labels\n",
    "train_dset = MNISTDset(images=train_data, labels=train_labels)\n",
    "train_loader = DataLoader(dataset=train_dset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "print(\"actual train_data count =\", train_data.size)\n",
    "print(\"actual train_labels count =\", train_labels.size)\n",
    "\n",
    "# val data\n",
    "val_data = data[dataTrainCount:] # takes the last 20% of the numpy array as validation set\n",
    "val_labels = labels[dataTrainCount:] # same for labels\n",
    "val_dset = MNISTDset(images=val_data, labels=val_labels)\n",
    "val_loader = DataLoader(dataset=val_dset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"actual val_data count=\", val_data.size)\n",
    "print(\"actual val_labels count =\", val_labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4219c-b4af-46a8-9a9b-6cf2a4491644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# implement main loop \n",
    "def main( learning_rates:list, batch_size:list, activation_functions:list):\n",
    "    num_epochs = 20\n",
    "    #model = NonLinearModel(input_dim=28*28, n_intermediate=2, intermediate_dim=50, act_fun=nn.ReLU(), output_dim=10)\n",
    "    model = CNN()\n",
    "    model.zero_init()\n",
    "    print(model)\n",
    "    optimizer = optim.Adam(params=model.parameters(),lr=0.001)\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    \n",
    "    device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    tr_loss = []\n",
    "    tr_acc = []\n",
    "    ev_loss = []\n",
    "    ev_acc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train(model, train_loader, optimizer, ce_loss, device, epoch)\n",
    "        tr_loss.append(loss)\n",
    "        \n",
    "        \n",
    "        # calculate accuracy\n",
    "        model.eval()\n",
    "        N = 2000\n",
    "        x, label, idx = train_dset[:N] \n",
    "        x = x.view(N,1,28,28) if model.is_conv else  x.view(N,-1) \n",
    "        out = model(x)\n",
    "        acc_ = (out.argmax(-1) == label).float().sum()/len(label)\n",
    "        tr_acc.append(acc_)\n",
    "\n",
    "\n",
    "        x, label, idx = val_dset[:N] \n",
    "        x = x.view(N,1,28,28) if model.is_conv else  x.view(N,-1)\n",
    "        model.eval()\n",
    "        out = model(x)\n",
    "        acc_ = (out.argmax(-1) == label).float().sum()/len(label)\n",
    "        ev_acc.append(acc_)\n",
    "        \n",
    "        \n",
    "        print(f'epoch [{epoch+1}/{num_epochs}]: train loss = {loss:.5f}, train acc = {tr_acc[-1]:.5f}, val acc = {ev_acc[-1]:.5f}')\n",
    "    \n",
    "    plt.plot(tr_loss, label='train loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(tr_acc, label='train accuracy')\n",
    "    plt.plot(ev_acc, label='eval accuracy')\n",
    "    plt.title('acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57667497-c3f8-40e2-bc50-72c935e02322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lecture')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfdd3e841bfae7d0ce1e26b4e00b5a6aed2f3fe7185f859880ab8fd14b7d0bf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
