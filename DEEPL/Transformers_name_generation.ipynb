{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194d084-8597-432d-ba53-ee71b5efaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install names-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94583c4c-8847-43c1-b111-b356f6da9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from names_dataset import NameDataset, NameWrapper\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b48a5-a2d9-42df-81d4-7cff398bb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDs(Dataset):\n",
    "    def __init__(self, names: list, seq_len: int, lookup: dict = None) -> None:\n",
    "        letters = sorted(list(set(''.join(names).lower())))\n",
    "        self.seq_len = seq_len\n",
    "        self.names = names\n",
    "        self.start_token = 'bos'\n",
    "        self.end_token = 'eos'\n",
    "        self.padding_token = 'pad'\n",
    "        self.lookup = lookup\n",
    "        if lookup is None:\n",
    "            self.lookup = {}\n",
    "            for i in range(len(letters)):\n",
    "                self.lookup[letters[i]] = i\n",
    "            self.lookup[self.start_token] = len(letters)\n",
    "            self.lookup[self.end_token] = len(letters) + 1\n",
    "            self.lookup[self.padding_token] = len(letters) + 2\n",
    "        \n",
    "            self.idx2letter = self._idx2letter()\n",
    "        \n",
    "    \n",
    "    def _idx2letter(self) -> dict:\n",
    "        '''create lookup dictionary num --> char'''\n",
    "        return {self.lookup[k]:k for k in self.lookup}\n",
    "    \n",
    "    def num_letters(self) -> int:\n",
    "        '''dictionary lenght'''\n",
    "        return len(self.lookup)\n",
    "    \n",
    "    def get_start_token(self) -> int:\n",
    "        '''get numeric value of start token'''\n",
    "        return self.lookup[self.start_token]\n",
    "    \n",
    "    def get_end_token(self) -> int:\n",
    "        '''get numeric value of end token'''\n",
    "        return self.lookup[self.end_token]\n",
    "        \n",
    "    def get_padding_token(self) -> int:\n",
    "        '''get numeric value of padding token'''\n",
    "        return self.lookup[self.padding_token]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        '''num of samples in dataset'''\n",
    "        return len(self.names)\n",
    "    \n",
    "    def idx2name(self, idx: torch.Tensor) -> str:\n",
    "        '''transform index array to name'''\n",
    "        ret = []\n",
    "        for i in idx:\n",
    "            if i == self.get_end_token():\n",
    "                break\n",
    "            elif i == self.get_start_token():\n",
    "                continue\n",
    "            ret.append(self.idx2letter[i.item()])\n",
    "        return ''.join(ret)\n",
    "    \n",
    "    def name2idx(self, name: str) -> torch.Tensor:\n",
    "        '''transform name to torch integer array'''\n",
    "        chars = list(name)\n",
    "        ret = [self.lookup[self.start_token]] + [self.lookup[c] for c in chars] + [self.lookup[self.end_token]]\n",
    "        assert self.seq_len >= len(ret), 'sequnce length exceeds maximal sequence length'\n",
    "        ret = ret + [self.lookup[self.padding_token]] * (self.seq_len - len(ret))  \n",
    "        ret = torch.tensor(ret).long()\n",
    "        return ret\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor]:\n",
    "        '''prepare sample'''\n",
    "        name = self.names[idx].lower()\n",
    "        ret = self.name2idx(name)\n",
    "        return ret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6003f-51c2-4f4c-8411-cd225ebc340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(2,3,4,4)\n",
    "M = (torch.ones(4,4) * float(\"-inf\")).triu(1)\n",
    "A += M\n",
    "A.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42406996-24de-4a8c-9048-594a71fc6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    '''config class containing arguments for transformers'''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.d = 16\n",
    "        self.input_dim = 16\n",
    "        self.conditional = True\n",
    "        self.n_heads = 4\n",
    "        self.n_layers = 2\n",
    "        self.dict_size = None\n",
    "        self.out_dim = None\n",
    "        self.max_seq_len = 50\n",
    "        self._set_args(**kwargs)\n",
    "    \n",
    "    def _set_args(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            assert hasattr(self, key), f'Invalid argument: {key}'\n",
    "            setattr(self, key, value)\n",
    "        assert self.dict_size is not None, f'no value for dict_size provided! (dict_size has not default value)'\n",
    "\n",
    "        \n",
    "class SimpleSelfAttentionHead(nn.Module):\n",
    "    '''Implementation of self attention head'''\n",
    "    def __init__(self, config):\n",
    "        super(SimpleSelfAttentionHead, self).__init__()\n",
    "        \n",
    "        self.d = config.d\n",
    "        self.input_dim = config.input_dim\n",
    "        self.beta = 1./np.sqrt(self.d)\n",
    "        \n",
    "        self.mapQ = nn.Parameter(torch.empty(self.input_dim, self.d).normal_()*0.02)\n",
    "        self.mapK = nn.Parameter(torch.empty(self.input_dim, self.d).normal_()*0.02)\n",
    "        self.mapV = nn.Parameter(torch.empty(self.input_dim, self.d).normal_()*0.02)\n",
    "        \n",
    "        self.use_mask = config.conditional\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Q = X @ self.mapQ\n",
    "        K = X @ self.mapK\n",
    "        V = X @ self.mapV\n",
    "        \n",
    "        A = Q @ K.permute(0,2,1) # keep batch size at first dimension\n",
    "        A = A * self.beta\n",
    "        if self.use_mask:\n",
    "            seq_len = A.shape[-1]\n",
    "            if self.mask is None or self.mask.shape[-1] != seq_len or self.mask.device != A.device:\n",
    "                self.mask = (torch.ones(seq_len, seq_len) * float(\"-inf\")).triu(1)\n",
    "                self.mask = self.mask.to(X.device)\n",
    "            A.data += self.mask\n",
    "            \n",
    "        A = A.softmax(dim=-1)\n",
    "        \n",
    "        V = A @ V\n",
    "        return V\n",
    "        \n",
    "        \n",
    "class SimmpleMultiheadAttention(nn.Module):\n",
    "    '''Compose multiple self attention heads to multi attention head'''\n",
    "    def __init__(self, config):\n",
    "        super(SimmpleMultiheadAttention, self).__init__()\n",
    "        \n",
    "        self.n_heads = config.n_heads\n",
    "        self.heads = nn.ModuleList([SimpleSelfAttentionHead(config) for _ in range(self.n_heads)])\n",
    "        self.project = nn.Parameter(torch.empty( config.d * self.n_heads, config.input_dim).normal_()*0.02)\n",
    "        self.act = torch.nn.GELU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        heads = [ h(X) for h in self.heads]\n",
    "        heads = torch.cat(heads, dim=-1)\n",
    "        out = heads @ self.project\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "    '''Multi attention heads + linear layers and layer norms'''\n",
    "    def __init__(self, config):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.feed_forward = nn.Linear(config.input_dim, config.input_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(config.input_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(config.input_dim)\n",
    "        self.attention = SimmpleMultiheadAttention(config)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.attention(X)\n",
    "        out = out + X\n",
    "        out = self.layernorm1(out)\n",
    "        out_f = self.feed_forward(out)\n",
    "        out = out_f + out\n",
    "        out = self.layernorm2(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class TransfomreEmbedding(nn.Module):\n",
    "    '''word embedding + positional encoding'''\n",
    "    def __init__(self, config):\n",
    "        super(TransfomreEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.dict_size, config.input_dim)\n",
    "        # TODO!\n",
    "        self.positional_encoding = nn.Parameter(torch.empty(config.max_seq_len, config.d).normal_()*0.02)\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        X = self.embedding(idx)\n",
    "        X = X + self.positional_encoding\n",
    "        return X\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    '''build the model but without output head'''\n",
    "    def __init__(self, config):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.dict_size = config.dict_size\n",
    "        self.embedding = TransfomreEmbedding(config)\n",
    "        self.n_layers = config.n_layers\n",
    "        self.layers = nn.ModuleList([AttentionLayer(config) for _ in range(self.n_layers)])\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        X = self.embedding(idx)                                     \n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    \n",
    "class ManyToManyHead(nn.Module):\n",
    "    '''Output head for many to many task'''\n",
    "    def __init__(self, config):\n",
    "        super(ManyToManyHead, self).__init__()\n",
    "        self.out_layer = nn.Linear(config.input_dim, config.out_dim)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.out_layer(X)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ManyToManyTransformer(nn.Module):\n",
    "    '''Combine base model and head to many to many model'''\n",
    "    def __init__(self, config):\n",
    "        super(ManyToManyTransformer, self).__init__()\n",
    "        self.config = config\n",
    "        self.simple_transformer = SimpleTransformer(config)\n",
    "        self.out_head = ManyToManyHead(config)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        out = self.simple_transformer(idx)\n",
    "        out = self.out_head(out)\n",
    "        return out\n",
    "    \n",
    "    def generate(self, bos_idx: int, eos_idx: int, pad_idx: int, start_idx: torch.Tensor=None, max_iter: int=10, from_top_k: int=1) -> List:\n",
    "        # input/output idx\n",
    "        #input_\n",
    "        \n",
    "        # in case of initial characters (see below) we need to check if \"eos\" token is already\n",
    "        # in the sequence - in this case, we do not need any prediction\n",
    "        has_finished = False \n",
    "        \n",
    "        # if we have already some initial characters (i.e. the task is to complete a name\n",
    "        seq = (torch.ones(self.config.max_seq_len) * pad_idx).long()\n",
    "        start = None\n",
    "        if start_idx is not None:\n",
    "            seq[:len(start_idx)] = start_idx\n",
    "            start = len(start_idx)\n",
    "        else:\n",
    "            # start sequence with \"bos\" token\n",
    "            seq[0] = bos_idx\n",
    "            start = 1\n",
    "            \n",
    "        # predict (rest of) the sequence\n",
    "        for i in range(max_iter):\n",
    "            out = self(seq[None])\n",
    "            seq[start + i] = out[0,i].topk(from_top_k).indices[torch.rand(from_top_k).argmax()]\n",
    "            if seq[start + i].item() == eos_idx:\n",
    "                break\n",
    "        \n",
    "        return seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb01233-ba5b-4c3d-8724-dcb1dc311e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775031c-9a81-4831-9e4d-80da47d5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(dict_size=10, out_dim=10, conditional=True)\n",
    "model = ManyToManyTransformer(config)\n",
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b1efd-0873-40af-aefd-2f2ae7ece641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_size(model.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e7306-c8ef-4de2-9d81-55239d815549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdd9fd-5b5c-4aa9-9ab6-484f5c03ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(names, p_train):\n",
    "    n_train = int(len(names) * p_train)\n",
    "    # shuffle data\n",
    "    idx = random.sample(range(len(names)), len(names))\n",
    "    train_data = [names[i] for i in idx[:n_train]]\n",
    "    val_data = [names[i] for i in idx[n_train:]]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5d0ec-8e37-4957-b5d0-29cc9da95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read names dataset and convert to lookup \n",
    "nd = NameDataset()\n",
    "names = []\n",
    "top_names = nd.get_top_names()\n",
    "# use all first names of all countries\n",
    "for i, countries in enumerate(top_names):\n",
    "    print(f'\\r reading names from country [{i+1}/{len(top_names)}]', end=\"\")\n",
    "    names += top_names[countries]['M']\n",
    "    names += top_names[countries]['F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a84ca-c752-4ae7-b6f9-f8cdf01ec7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nd = NameDataset()\n",
    "#names = nd.get_top_names(country_alpha2='AT')['AT']\n",
    "#names = names['M'] + names['F']\n",
    "# train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11bf97f-bd74-4266-a368-9df359b7c6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a71f53-7b2a-42da-8872-1113160e31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, dataloader, model, loss_func, optimizer, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    loss_ls = []\n",
    "    n_samples = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        bs = batch.shape[0]\n",
    "        n_samples += bs\n",
    "        model.zero_grad()\n",
    "        out = model(batch)\n",
    "        \n",
    "        # note: we need to compare the t'th output with the (t+1)'th token in our batch!\n",
    "        # loss = loss_func(out[:,:-1].reshape(-1, model.config.dict_size), batch[:,1:].reshape(-1))\n",
    "        loss = loss_func(out[:,:-1].reshape(-1, model.config.dict_size), batch[:,1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()/bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cae72-7f43-4aef-af00-3d9b507414f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch, dataloader, model, loss_func, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    n_samples = 0\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            loss = loss_func(out[:,:-1].reshape(-1, model.config.dict_size), batch[:,1:].reshape(-1))\n",
    "            #print(f'val loss: {loss}')\n",
    "            loss_sum += loss.item()\n",
    "            n_samples += batch.shape[0]\n",
    "            #print(n_samples)\n",
    "    avg_loss = loss_sum/n_samples\n",
    "    return avg_loss                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be95625-a0d4-474f-b4cd-b7e07d713e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_epochs=20):\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    seq_len = 50\n",
    "\n",
    "    # create split\n",
    "    train_split, val_split = split_data(names, 0.75)\n",
    "\n",
    "    # create datasets\n",
    "    train_data = NamesDs(train_split, seq_len)\n",
    "    val_data = NamesDs(val_split, seq_len)\n",
    "\n",
    "    # create loader\n",
    "    train_loader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_data, batch_size=266, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # create loss\n",
    "    loss_fun = nn.CrossEntropyLoss(ignore_index=train_data.get_padding_token(), size_average=True)\n",
    "    \n",
    "    # instantiate model\n",
    "    dict_size = train_data.num_letters()\n",
    "    config = Config(dict_size=dict_size, \n",
    "                    out_dim=dict_size, \n",
    "                    d=128, \n",
    "                    input_dim=128, \n",
    "                    conditional=True)\n",
    "    model = ManyToManyTransformer(config)\n",
    "    \n",
    "    # optimizer\n",
    "    # optim = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "    \n",
    "    epochs = n_epochs\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        t_loss = train(epoch, train_loader, model, loss_fun, optim, device=device)\n",
    "        v_loss = val(epoch, val_loader, model, loss_fun, device=device)\n",
    "        print(f\"\\repoch: [{epoch}/{n_epochs}]: train_loss = {t_loss:.5f} | val_loss = {v_loss:.5f}\", end=\"\")\n",
    "        train_loss.append(t_loss)\n",
    "        val_loss.append(v_loss)\n",
    "    return model, train_loss, val_loss, train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f232feb-bb9a-4e62-bfcb-d78714ec368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e442ed8-0ab1-4571-a880-a952c295ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, train_loss, val_loss, train_data, val_data = main(n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d2863-46f3-4f1a-9c88-ce8159614df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5809a8-76b9-4c5b-b5a2-8c0248b92ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(val_loss, label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8f1f8-1fa9-4516-a084-2428075da6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "out = model.generate(bos_idx=train_data.get_start_token(),\n",
    "                     eos_idx=train_data.get_end_token(),\n",
    "                     pad_idx=train_data.get_padding_token(),\n",
    "                     from_top_k=5)\n",
    "train_data.idx2name(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58949c2e-0e84-4a6e-beaa-74036da1e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = 'ju'\n",
    "s = train_data.name2idx(input_str)[:len(input_str)+1]\n",
    "print(len(s))\n",
    "out = model.generate(bos_idx=train_data.get_start_token(), \n",
    "                     eos_idx=train_data.get_end_token(),\n",
    "                     pad_idx=train_data.get_padding_token(),\n",
    "                     start_idx=s, from_top_k=5)\n",
    "train_data.idx2name(torch.tensor(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730691c-e2ec-478b-97e8-c0315ee247dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
